# -*- coding: utf-8 -*-
"""SubRedditScrapper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16QgpLoDgKHM87XhxnbTu81XWfWGnkDd9
"""

import praw
import json
from datetime import datetime

USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36"

reddit = praw.Reddit(
    client_id='xxxxxx',
    client_secret='xxxxxxx',
    user_agent=USER_AGENT,
)

def convert_date(timestamp):
    return datetime.utcfromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')

def scrape_subreddit(subreddit_name, num_posts):
    subreddit = reddit.subreddit(subreddit_name)

    posts_data = []

    for post in subreddit.hot(limit=num_posts):
        if post.selftext and len(post.selftext.strip()) > 50:
            post_data = {
                'title': post.title,
                'content': post.selftext.strip(),
                'date': convert_date(post.created_utc)
            }
            posts_data.append(post_data)

    return posts_data

def scrape_multiple_subreddits(subreddit_list, num_posts):
    for subreddit_name in subreddit_list:
        print(f"Scraping r/{subreddit_name} ...")
        posts = scrape_subreddit(subreddit_name, num_posts)

        with open(f'{subreddit_name}_posts.json', 'w', encoding='utf-8') as f:
            json.dump(posts, f, ensure_ascii=False, indent=4)

        print(f"Saved {len(posts)} posts from r/{subreddit_name} to {subreddit_name}_reddit_posts.json.")

subreddit_list = ['pittsburgh', 'steelers', 'buccos', 'penguins']

num_posts = 500

scrape_multiple_subreddits(subreddit_list, num_posts)

