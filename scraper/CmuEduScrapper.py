# -*- coding: utf-8 -*-
"""CmuEduScrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15xgLxJxyVpqEQKfqRWUVVa4O6c2_PeCU
"""

import torch
from transformers import pipeline
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

model_id = "meta-llama/Llama-3.2-1B-Instruct"
pipe = pipeline(
    "text-generation",
    model=model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    max_new_tokens=128,
)

visited_urls = set()

visited_lock = threading.Lock()
file_lock = threading.Lock()
counter_lock = threading.Lock()

processed_links = 0
MAX_LINKS = 100

def process_text_with_llm(text, url):
    """
    Process the scraped text using the language model.
    """
    messages = [
        {"role": "system", "content": "You are a language model that cleans up and summarizes text without losing important information."},
        {"role": "user", "content": text},
    ]

    try:
        output = pipe(messages[1]["content"], max_new_tokens=256)
        cleaned_text = output[0]["generated_text"]

        with file_lock:
            with open('processed_text.txt', 'a', encoding='utf-8') as f:
                f.write(f"{cleaned_text.strip()}\n")
                f.write(f"{'-'*80}\n\n")

    except torch.cuda.OutOfMemoryError:
        torch.cuda.empty_cache()
        print(f"Out of memory when processing URL: {url}")
    except Exception as e:
        print(f"Error during LLM processing: {e}")

def scrape_text(url, max_depth=3, depth=0):
    """
    Recursively scrape text from the given URL and its child links.
    """
    global processed_links

    with counter_lock:
        if processed_links >= MAX_LINKS:
            return

    with visited_lock:
        if url in visited_urls or depth > max_depth:
            return
        visited_urls.add(url)

    print(f"Processing link {processed_links+1}/{MAX_LINKS}: {url}")

    try:
        headers = {'User-Agent': 'Mozilla/5.0 (compatible; WebScraper/1.0)'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        encoding = response.encoding or 'utf-8'
        page_content = response.content.decode(encoding, errors='replace')

        soup = BeautifulSoup(page_content, 'html.parser')

    except requests.RequestException as e:
        print(f"Failed to retrieve {url}: {e}")
        return
    except UnicodeDecodeError as e:
        print(f"Encoding error at {url}: {e}")
        return

    page_text = soup.get_text(separator=' ', strip=True)
    page_text = page_text.encode('ascii', 'ignore').decode('utf-8')

    with counter_lock:
        processed_links += 1

    process_text_with_llm(page_text, url)
    with counter_lock:
        if processed_links >= MAX_LINKS:
            return

    links_to_scrape = []
    for link_tag in soup.find_all('a', href=True):
        href = link_tag['href']
        child_url = urljoin(url, href)

        if urlparse(child_url).netloc != urlparse(url).netloc:
            continue

        with counter_lock:
            if processed_links >= MAX_LINKS:
                break

        links_to_scrape.append(child_url)

    return links_to_scrape

def main(starting_url):
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = {executor.submit(scrape_text, starting_url): starting_url}

        while futures and processed_links < MAX_LINKS:
            for future in as_completed(futures):
                try:
                    new_links = future.result()
                    if new_links:
                        for link in new_links:
                            if processed_links >= MAX_LINKS:
                                break
                            futures[executor.submit(scrape_text, link)] = link

                except Exception as exc:
                    print(f"Error during scraping: {exc}")

                del futures[future]

if __name__ == "__main__":
    starting_url = input("Enter the URL to scrape: ")
    with open('processed_text.txt', 'w', encoding='utf-8') as f:
        f.write('')
    main(starting_url)

